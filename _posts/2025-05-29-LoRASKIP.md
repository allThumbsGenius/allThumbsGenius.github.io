---
layout: post
title: "LoRASKIP: LoRA Self Knowledge-dIstilled Pruning"
description: "Tried novel method for pruning using LoRA"
modified: 2025-05-29
tags: [Efficient AI]
image:
  path: /images/loraskip/LoRASKIP_conceptmap.png
  feature: LoRASKIP_conceptmap.png
---

# 1. Background
---

### 1. Motivation: What we actually do while studying further more?

We can say the **fine-tuning** and **pruning** are both motivated by the **human brain’s behavior**. First, **fine-tuning** can be thought **applying previous knowledge to new knowledge**. It can help us adapt fast and well to the new task. Second, **pruning** is from biological motivation, which is ****our brain **prunes the useless neuron** after get use to some task.

In the machine learning area, **most researches think and apply fine-tune and prune models separately** (sure, there is a method that trains and prunes simultaneously using sparsity regularization but does not on fine-tune). But **what “we” actually do??** Does these methods reflect our and our brain’s behavior?? Assume that we have already studied some subject (like linear algebra). Actually, when we study that subject, we decide the **importance of the content** and **only study important content**. The more we study, the **less unimportant content is left for study**. And **finally, we only concentrate on and study the important contents**. Plus, we are **not deciding the importance** of content related to the subject **while resting**. If we do so, it may be just **FORGETTING**, not further studying. If you want to make neural network follow the behavior of our brain, why do you let fine-tune and pruning separately? At the end, we think **pruning is in the area of fine-tuning;** thus we do both **simultaneously**.

![Fig 1. Neuron pruning on children’s brain[^1]]({{ site.url }}/images/loraskip/pruning_motivation.png)

[^1]: C. A. Walsh, “Peter Huttenlocher (1931-2013),” Nature, 2013

A simple brief of our method: we use **LoRA**, **MaskLoRA** and **self-knowledge distillation**. **LoRA** is for fine-tuning to concentrate important weight, and **MaskLoRA** is for pruning that removes unimportant weight. And finally, we use **self-knowledge distillation** to prevent forgetting original knowledge. The original model’s output applies as a soft label, which contains rich information of the original model.

### 2. LoRA (인용 넣기)

LoRA is **Parameter Efficient Fine-Tuning(PEFT)** method. The author of the LoRA is inspired by the idea that model that have huge parameters lie in a low intrinsic dimension. So, they make the model **only fine-tune a few parameters** which are **relevant with intrinsic low rank**. Practically, they add low-rank parameters that can construct the same size as the model weight with multiplication and fine-tune low-rank parameters only while freezing pre-trained weight.

![Fig 2. LoRA method](image.png)

Fig 2. LoRA method

In more detail, original fine-tuning updates weight as follow.

$$
W'=W_{0}+\Delta W
$$

To lower the training parameter, LoRA decomposes updating weight to A and B where $A \in \mathbb{R}^{r \times k}$, $B \in \mathbb{R}^{d \times r}$ ($k$ is the input dimension and $d$ is the output dimention).

$$
\Delta W = BA
$$

Now they train only A and B. Someone can think, ‘Is the output the same as the original fine-tuning?’ But yes, it is. They simply show this with distribution law.

$$
output = W'x=(W_0+\Delta W)x=W_0x+BAx
$$

For more detail, please see the original paper (인용넣기).

### 3. Pruning

Pruning is **removing parameters that have low importance**. A model that is done with training may have some unnecessary or redundant parameters that increase usage of memory and operations while not affecting the model’s performance. Pruning improves **inference speed** and **compresses model size** by removing these parameters. There are **many methods available for pruning** the model. For example, we can prune **before training,** **during training,** or **after training,** etc. From here, we are going to use the idea of **pruning during training,** which puts **sparsity regularization on the loss term**. This will be explained in the next chapter. For more detail on pruning, we will leave a survey paper for you (인용 넣기).

![Fig 3. An overview of the hierarchical structure of the survey on pruning (인용 넣기)](image%201.png)

Fig 3. An overview of the hierarchical structure of the survey on pruning (인용 넣기)

# 2. Method

---

### 1. LoRA & MaskLoRA

![Fig 4. Overview of the proposed framework.](LoRASKIP_conceptmap.png)

Fig 4. Overview of the proposed framework.

We extend the standard Low-Rank Adaptation (LoRA) framework by introducing a structured masking mechanism that enables learnable sparsification of LoRA weights. This extension, which we refer to as **MaskLoRA**, equips the model with the ability to simultaneously learn what to update—via low-rank residuals—and where to update—via a structured binary mask over the adaptation weights.

Standard LoRA enhances a frozen pre-trained weight $W_0$ by adding a learnable low-rank residual $(AB)^\top$, scaled by a factor $\gamma$:

$$
\hat{W}=W_0+\gamma\cdot (AB)^\top
$$

In our proposed method, we modulate this residual with a **binary mask** $M\in\mathbb{R}^{ d_h \times d_\text{model}}$, yielding:

$$
\hat{W} = (W_0 + \gamma \cdot (AB)^\top) \odot M
$$

where $d_\text{model}$ is the dimensionality of the input token embeddings, and $d_h$ denotes the projection dimension per attention head.

To encourage structured sparsity, the mask $M$ is constructed as the outer product of two vectors:

- $A_{\text{mask}} \in \mathbb{R}^{d_\text{model} \times 1}$, a fixed vector of ones registered as a non-learnable buffer,
- $B_{\text{mask}} \in \mathbb{R}^{1 \times d_h}$, a learnable row vector optimized during training.

The resulting mask $M$ shares the same pruning pattern across all rows, effectively applying a channel-wise masking strategy. This design simplifies optimization and enables coarse-grained control over which channels in the LoRA update are retained.

In our implementation, the **mask is applied** specifically to the **query and value projection matrices** in each self-attention block. This decision follows the original LoRA paper (인용 넣기 [Hu et al., 2021]), which showed that fine-tuning just the query and value projections is sufficient for effective adaptation in transformer-based architectures, capturing most of the performance benefits with minimal overhead.
While **channel-wise pruning** does not necessarily guarantee a corresponding reduction in inference latency—especially given the block-structured memory access and kernel fusion behavior of modern accelerators—the **learned sparsity pattern provides valuable insight** into the model’s internal adaptation behavior. 

Pruning a specific channel in the query projection removes the need to consider the corresponding channel in the key projection. Therefore, the successful **pruning in the query projection** may imply that **computing attention weights does not necessarily require contributions** from all channels. On the other hand, **pruning a value projection** channel could imply that the variation introduced by the attention mechanism along that **output direction is unnecessary for the subsequent MLP processing**.  In such cases, the model **instead relies on the residual connection** to propagate information, implying that the unmodified input is already sufficient to represent that semantic subspace.

In our implementation, masked channels are simulated by explicitly zeroing out their corresponding dimensions during inference, thereby preserving functional equivalence while avoiding custom low-level kernels. Even though this does not reduce actual computation, **it exposes a meaningful sparsity pattern** that can be interpreted and potentially exploited by downstream compression techniques.

Also, to convert the real-valued outer product $A_{\text{mask}} B_{\text{mask}}$ into a binary mask, we apply a **sigmoid activation** followed by hard thresholding at a fixed value (e.g., 0.5). Because this thresholding operation is non-differentiable, we adopt the **Straight-Through Estimator (STE)** during training. STE uses the hard-threshold during the forward pass but approximates the gradient as if the sigmoid function had been applied directly, allowing smooth end-to-end optimization of the mask parameters despite their discrete behavior at inference.

Therefore, the final binary mask is constructed as follows:

$$
M= \text{STE}(\sigma((A_{\text{mask}} B_{\text{mask}})^\top))
$$

Through this formulation, **MaskLoRA** unifies two essential objectives in parameter-efficient fine-tuning: **performance** and **sparsity**. It enables the model to adapt effectively while selectively suppressing redundant updates in a structured and interpretable manner.

### 2. Loss formulation

To guide the masked adaptation process, we design a composite **loss function** composed of three components: a **task-specific classification loss**, a **distillation loss** from the teacher model, and a **sparsity-inducing regularization term** on the learned mask.

Let $f_{\text{student}}(x) \in \mathbb{R}^C$ and $f_{\text{teacher}}(x) \in \mathbb{R}^C$ denote the logits produced by the student and teacher models respectively for an input image $x$, where $C$ is the number of classes. Let $\mathbf{y} \in \{0,1\}^C$ be the one-hot encoded ground-truth label vector, and $T > 0$ be a temperature parameter. The **total loss** is defined as:

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{CE}} + \alpha \cdot \mathcal{L}_{\text{distill}} + \beta \cdot \mathcal{L}_{\text{sparsity}}
$$

The **classification loss** $\mathcal{L}_{\text{CE}}$ is the standard **cross-entropy loss** computed between the student’s predicted distribution and the one-hot label:

$$
\mathcal{L}_{\text{CE}} = -\sum_{c=1}^Cy_c\log \left(\text{softmax}(f_\text{student})_c \right)
$$

The **distillation loss** $\mathcal{L}_{\text{distill}}$ encourages the student model to **preserve the soft output behavior of the original model** before masking was applied. It is computed via the **Kullback–Leibler divergence** between the temperature-scaled output distributions:

$$
\mathcal{L}_{\text{distill}} = T^2 \cdot \text{KL} \left( \text{softmax} \left( \frac{f_{\text{teacher}}}{T} \right) \Big\| \text{softmax} \left( \frac{f_{\text{student}}}{T} \right) \right)
$$

In our setting, the **teacher** is not a separate model but the **unmasked version of the student itself**—that is, the same backbone before any masking or pruning is applied. This constitutes a form of **self-distillation**, where the model is trained to **retain its original predictive behavior** even as adaptation and sparsification are introduced. Such regularization helps mitigate the risk of overfitting to the task-specific data and encourages the adapted model to remain close to the original model’s semantic manifold.

The final term, $\mathcal{L}_{\text{sparsity}}$ promotes **mask sparsity**. Specifically, for each transformer block $i$, let $M_{i,q} \in [0, 1]^{ d_h \times d_\text{model}}$ and $M_{i,v} \in [0, 1]^{d_h \times d_\text{model}}$ denote the **soft masks**—i.e., the **output of the sigmoid activations**—applied to the query and value projections, respectively. The sparsity loss is computed as the sum of normalized **$L_{1,1}$ norm** of each mask:

$$
\mathcal{L}_{\text{sparsity}} = \sum_{i=1}^L \frac{1}{2}\cdot\left( \frac{1}{d_\text{model}d_h} \sum_{j,k} |M_{i,q}^{(j,k)}| + \frac{1}{d_\text{model}d_h} \sum_{j,k} |M_{i,v}^{(j,k)}| \right)
$$

where $L$ is the number of transformer blocks. This term encourages most mask values to approach zero, thereby pruning redundant adaptation directions, while leaving only a small subset of critical subspaces for task-specific fine-tuning.

Together, the **three terms** define an objective that balances **task performance**, **fidelity to the original model behavior**, and **structural efficiency** in adaptation. By jointly optimizing this loss, the model is encouraged to focus its limited adaptation capacity on dimensions that truly matter, while avoiding unnecessary updates and preserving generalization.

### 3. Training strategy

All experiments were conducted using a **DeiT-Base model** as the backbone. Before conducting full training, we performed **hyperparameter tuning using Optuna**, a widely adopted hyperparameter optimization library based on Bayesian optimization with a Tree-structured Parzen estimator (TPE). Internally, Optuna approximates the posterior over promising regions using a Gaussian mixture model (GMM), and selects sampling candidates by maximizing the expected improvement over observed trials. This approach enables efficient exploration of high-dimensional, continuous search spaces even with limited trial budgets.

We used Optuna to **tune four key hyperparameters**: the **LoRA rank** $r$, the **distillation loss coefficient** $\alpha$, the **distillation temperature** $T$, and the **LoRA scaling factor** $\gamma$. The corresponding search spaces were defined as $r \in \{2, \dots, 8\}$, $\alpha \in [0.1, 1.0]$, $T \in [2.0, 5.0]$, $\gamma \in [0.1, 2.0]$ with all continuous variables explored using a search step size of 0.0001. The **sparsity regularization weight** $\beta$, on the other hand, was **fixed to one of four pre-selected values** $\{0.01, 0.09, 0.21, 1.00\}$, and hyperparameter optimization was performed independently for each setting of $\beta$. The objective of each Optuna trial was to **maximize validation accuracy**, measured not on the ImageNet validation set, but on a **subset of the ImageNet training set constructed by randomly sampling 50 images** per class from the 1000-class ImageNet-1k training split. To avoid confusion, we emphasize that **no part of the official ImageNet validation set was used in any stage of Optuna tuning**.

**Training** for each trial was conducted on a similarly **constructed 50-shot subset** of the ImageNet training set (i.e., **50 samples per class** drawn at random with fixed seed), using a **maximum of 10 training epochs**. Although both the number of epochs and total trials were relatively small, this was a practical choice to manage computational cost. We did not attempt to exhaustively search for the best setting, but **aimed to identify a set of reasonably effective hyperparameters** for use in subsequent fine-tuning.

Among the trials on each fixed-$\beta$ setting, we identified **Pareto-optimal candidates balancing validation accuracy and mask sparsity**—measured by the proportion of entries in the learned binary mask that fell below the hard threshold of 0.5. From the resulting Pareto fronts, we selected **four configurations** that reflected different trade-offs between accuracy and sparsity, with the chosen hyperparameters as follows:

![LoRA (1)-1.png](LoRA_(1)-1.png)

**Using these hyperparameters, we conducted full training** on the entire ImageNet-1k training set. The optimization proceeded in **two phases**. In the **first phase**, both the **LoRA weights and the mask parameters were jointly optimized for 10 epochs**. This joint training phase allowed the model to discover which channels were most relevant for adaptation, while learning a soft mask that reflects the structure of redundant or non-essential adaptation dimensions. In the **second phase**, the **learned mask was frozen for all mask parameters**, and training continued for an **additional 10 epochs** with updates restricted to the **LoRA weights only**. This decoupled training strategy encourages **stability**, and allows the model to **refine the adaptation** within the support already selected by the mask. Furthermore, this process can be considered as **“pretrained model: study all concepts”**, **“self-distilled pruning & fine-tuning: highlighting important concepts”**, **“freeze mask & further tuning: study with only important concepts”** which humans do.

We used the Adam optimizer with a learning rate of $10^{-4}$ and a batch size of 192 throughout the whole phase. However, for the configurations with $\beta = 0.21$ and $\beta = 1.00$, we observed instability in the loss during the second phase. To mitigate this, we reduced the learning rate to $10^{-5}$ in these cases, which led to more stable convergence.

# 3. Results

---

- [x]  masking rate (qv 합쳤을 때임) vs accuracy
- [x]  다른  method랑 우리 거랑 같이 점으로 plot한거 (parameter vs accuracy): 이건 제가 하겠습니다~! - 현우
- [x]  각 layer 별로 DeiT-Base 얼마 정도 pruning 되었는지
- [ ]  high masking rate에서는 subset으로 training했을 때 (요거 기준은 추가 fine tuning 없었을 때임) validation accuracy 높았다ㅠㅠ (이유 대충 추측)
- [x]  fine tuning 하기 전 accuracy vs fine tuning 한 후 accuracy → 추가 fine tuning의 위력은? (나빠도 여기다 적자)

![masking vs acc.jpg](masking_vs_acc.jpg)

![LoRA-2.png](LoRA-2.png)

![LoRA-3.png](LoRA-3.png)

![LoRA-4.png](LoRA-4.png)

![LoRA-5.png](LoRA-5.png)

![two phase training.jpg](two_phase_training.jpg)

![image.png](image%202.png)

# 4. Limitation

---

There are some **limitations** on our method and claim. But we are happy that this means our method has more potential to further improve. Maybe you guys can find the next research idea from here!

- **Increasing parameter**
    - Although we are using the LoRA method for fine-tuning and pruning, two LoRA layers are needed, which makes more parameters.

- **Zero parameters are still there**
    - We are not removing parameters but setting zero using MaskLoRA, so the parameters are still there.

- **Hyperparameter searching**
    - We need to find optimal hyperparameters using Optuna.
    - We are using a subset to find hyperparameters but still need a bunch of time.
    - And saying using a subset, found hyperparameters have the possibility of not being optimal for the model.

- **Not enough experiment (Due to Not enough resources HaHa…)**
    - We only experienced on DeiT-Base model.
    - For tasks, we only tested on ImageNet classification task.

- **Relatively Bad Performance**
    - Compared with other methods, it has relatively bad performance.
    - We think this occurred since the lack of training epoch.

# 5. Conclusion

---

In this work, we explored LoRASKIP which is a **two-phase training strategy that jointly learns both LoRA weights and channel-wise binary masks**, with the goal of identifying task-relevant adaptation subspaces in vision transformers. To the best of our knowledge, this is the **first study** that reports empirical results on **simultaneously training weight updates and pruning masks** within a **Vision Transformer (ViT) backbone**. While prior works have considered LoRA-based adaptation and pruning-based compression separately, we investigated their joint optimization in a unified framework.

Our central **motivation** stemmed from a simple yet underexplored question: **imitating human behavior**, if structured pruning can be seen as imposing a learnable mask, **is it possible**—and perhaps even beneficial—**to optimize such a mask alongside weight adaptation?** In this view, **pruning** is not merely a conventional model compression method applied after pretraining, but **fundamentally the same optimization process as weight adaptation**, just operating under an explicit sparsity constraint.

Admittedly, the scope of **our investigation was limited by practical constraints**: we did not perform exhaustive comparisons across a wide range of baselines, model architectures, or datasets. As such, our findings cannot offer a definitive answer to the general utility of the approach. 

Nevertheless, we believe **this work remains meaningful**. Through a series of controlled experiments on ImageNet-1k, we observed that jointly learning masks and LoRA weights not only yields stable training behavior but also does not inherently degrade performance. We were initially concerned that this joint optimization might lead to unstable training or divergence, especially in early stages. However, the **training process remained stable**, and in several configurations, the joint approach proved surprisingly **robust**.

Moreover, our findings suggest a subtle yet intriguing phenomenon: in highly constrained training setups where aggressive pruning is applied but **further fine-tuning is impractical**, **using a reduced training set may paradoxically improve final performance**. This may be implying that **mask optimization may not always require prolonged training** to be effective.

The practical implications of our method—for instance, its potential to reduce inference latency or compress model size to an industrially meaningful degree—may remain **limited by current hardware constraints**. Nonetheless, this study highlights a direction worth exploring, not because it promises **immediate deployment** gains, but because **it contributes to our conceptual understanding of sparsity, adaptation, and model efficiency**. Even in the absence of overwhelming success, **negative results or inconclusive findings can guide future research** by clarifying which paths may be less fruitful and which hypotheses merit deeper investigation. We hope that the ideas explored here may inform future studies on structured pruning and adaptation in large-scale models such as LLMs, where balancing sparsity and task performance is of growing practical relevance.

# 회의

---

![Fig 5. illustration of typical sparsity changes from our method](image%203.png)

Fig 5. illustration of typical sparsity changes from our method

LLM에도 사용될 수 있다 이런거 써도 될지도요..?ㅋㅋㅋ

엇 이거 제 파트 아니었네요 ㅋㅋㅋㅋㅋㅋㅋㅋ;; 

아 이거 잠시만요

![image.png](image%204.png)

서베이 논문에서 이런식으로 나타내서 저도 한번 따라해봤어요. 이거 보시면 계단모양으로 iterative하게 pruning이랑 training이 더 진행되는데 저희는 finetuning으로 한번에 처리해서 사선으로 이루어져 있습니다.

![image.png](image%205.png)

수업 때에도 한번 언급하신 것 같아요!

**related works - pruning을 기존 방법으로 하고, 그 다음에 tuning하는 것과 비교를 못함 (이건 연구자-우리-들의 resource 이슈) (”이건 이미 서베이 논문에 있어서 없어도 저희가 안해도 되지 싶어요”)**

역할 분담:

이: Method, Conclusion

정: Background, Limitation

Result는 내일 빠르게 호로록

마지막에 서빈님 이름 넣어야 하는거 잊지 않기! (제 블로그라서 ㅋㅋㅋㅋ 안올리시면 저만 한게 된답니다?)

[poster.pptx](poster.pptx)

![LoRASKIP_conceptmap.png](LoRASKIP_conceptmap.png)