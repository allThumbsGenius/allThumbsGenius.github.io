---
layout: post
title: "LoRASKIP: LoRA Self-Knowledge-dIstilled Pruning"
description: "LoRASKIP: LoRA Self-Knowledge-dIstilled Pruning"
modified: 2025-05-29
tags: [Efficient AI]
image:
  path: /images/loraskip/LoRASKIP_conceptmap.png
  feature: /loraskip/LoRASKIP_conceptmap.png
---

# 1. Background
---

### 1. Motivation: What we actually do while studying further more?

We can say the **fine-tuning** and **pruning** are both motivated by the **human brain’s behavior**. First, **fine-tuning** can be thought **applying previous knowledge to new knowledge**. It can help us adapt fast and well to the new task. Second, **pruning** is from biological motivation, which is **our brain prunes the useless neuron** after get use to some task.

In the machine learning area, **most researches think and apply fine-tune and prune models separately** (sure, there is a method that trains and prunes simultaneously using sparsity regularization but does not on fine-tune). But **what “we” actually do??** Does these methods reflect our and our brain’s behavior?? Assume that we have already studied some subject (like linear algebra). Actually, when we study that subject, we decide the **importance of the content** and **only study important content**. The more we study, the **less unimportant content is left for study**. And **finally, we only concentrate on and study the important contents**. Plus, we are **not deciding the importance** of content related to the subject **while resting**. If we do so, it may be just **FORGETTING**, not further studying. If you want to make neural network follow the behavior of our brain, why do you let fine-tune and pruning separately? At the end, we think **pruning is in the area of fine-tuning;** thus we do both **simultaneously**.

|![Fig 1. Neuron pruning on children’s brain]({{ site.url }}/images/loraskip/pruning_motivation.png)|
|:--:|
|Fig 1. Neuron pruning on children's brain[^1]|

[^1]: C. A. Walsh, “Peter Huttenlocher (1931-2013),” Nature, 2013

A simple brief of our method **"LoRASKIP"**: we use **LoRA**, **MaskLoRA** and **self-knowledge distillation**. **LoRA** is for fine-tuning to concentrate important weight, and **MaskLoRA** is for pruning that removes unimportant weight. And finally, we use **self-knowledge distillation** to prevent forgetting original knowledge. The original model’s output applies as a soft label, which contains rich information of the original model.

### 2. LoRA [^lora]

[^lora]: E. J. Hu et al., “LoRA: Low-Rank Adaptation of Large Language Models,” Oct. 16, 2021, arXiv: arXiv:2106.09685. doi: 10.48550/arXiv.2106.09685.

LoRA is **Parameter Efficient Fine-Tuning(PEFT)** method. The author of the LoRA is inspired by the idea that model that have huge parameters lie in a low intrinsic dimension. So, they make the model **only fine-tune a few parameters** which are **relevant with intrinsic low rank**. Practically, they add low-rank parameters that can construct the same size as the model weight with multiplication and fine-tune low-rank parameters only while freezing pre-trained weight.

|![Fig 2. LoRA method]({{ site.url }}/images/loraskip/original_lora.png)|
|:--:|
|Fig 2. LoRA method|

In more detail, original fine-tuning updates weight as follow.

[^lora]: E. J. Hu et al., “LoRA: Low-Rank Adaptation of Large Language Models,” Oct. 16, 2021, arXiv: arXiv:2106.09685. doi: 10.48550/arXiv.2106.09685.


$$
W'=W_{0}+\Delta W
$$

To lower the training parameter, LoRA decomposes updating weight to A and B where $$A \in \mathbb{R}^{r \times k}$$, $$B \in \mathbb{R}^{d \times r}$$ ($$k$$ is the input dimension and $$d$$ is the output dimention).

$$
\Delta W = BA
$$

Now they train only A and B. Someone can think, ‘Is the output the same as the original fine-tuning?’ But yes, it is. They simply show this with distribution law.

$$
output = W'x=(W_0+\Delta W)x=W_0x+BAx
$$

For more detail, please see the original paper[^lora].

### 3. Pruning

Pruning is **removing parameters that have low importance**. A model that is done with training may have some unnecessary or redundant parameters that increase usage of memory and operations while not affecting the model’s performance. Pruning improves **inference speed** and **compresses model size** by removing these parameters. There are **many methods available for pruning** the model. For example, we can prune **before training,** **during training,** or **after training,** etc. From here, we are going to use the idea of **pruning during training,** which puts **sparsity regularization on the loss term**. This will be explained in the next chapter. For more detail on pruning, we will leave a survey paper for you[^pruning_survey].

[^pruning_survey]: H. Cheng, M. Zhang, and J. Q. Shi, “A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 12, pp. 10558–10578, Feb. 2024, doi: 10.1109/TPAMI.2024.3447085.

|![Fig 3. An overview of the hierarchical structure of the survey on pruning]({{ site.url }}/images/loraskip/pruning_survey.png)|
|:--:|
|Fig 3. An overview of the hierarchical structure of the survey on pruning[^pruning_survey]|

# 2. Method

---

### 1. LoRA & MaskLoRA

|![Fig 4. Overview of the proposed framework.]({{site.url}}/images/loraskip/LoRASKIP_conceptmap.png)|
|:--:|
|Fig 4. Overview of the proposed framework|

We extend the standard Low-Rank Adaptation (LoRA) framework by introducing a structured masking mechanism that enables learnable sparsification of LoRA weights. This extension, which we refer to as **MaskLoRA**, equips the model with the ability to simultaneously learn what to update—via low-rank residuals—and where to update—via a structured binary mask over the adaptation weights.

Standard LoRA enhances a frozen pre-trained weight $$W_0$$ by adding a learnable low-rank residual $$(AB)^\top$$, scaled by a factor $$\gamma$$:

$$
\hat{W}=W_0+\gamma\cdot (AB)^\top
$$

In our proposed method, we modulate this residual with a **binary mask** $$M\in\mathbb{R}^{ d_h \times d_\text{model}}$$, yielding:

$$
\hat{W} = (W_0 + \gamma \cdot (AB)^\top) \odot M
$$

where $$d_\text{model}$$ is the dimensionality of the input token embeddings, and $$d_h$$ denotes the projection dimension per attention head.

To encourage structured sparsity, the mask $M$ is constructed as the outer product of two vectors:

- $$A_{\text{mask}} \in \mathbb{R}^{d_\text{model} \times 1}$$, a fixed vector of ones registered as a non-learnable buffer,
- $$B_{\text{mask}} \in \mathbb{R}^{1 \times d_h}$$, a learnable row vector optimized during training.

The resulting mask $$M$$ shares the same pruning pattern across all rows, effectively applying a channel-wise masking strategy. This design simplifies optimization and enables coarse-grained control over which channels in the LoRA update are retained.

In our implementation, the **mask is applied** specifically to the **query and value projection matrices** in each self-attention block. This decision follows the original LoRA paper[^lora], which showed that fine-tuning just the query and value projections is sufficient for effective adaptation in transformer-based architectures, capturing most of the performance benefits with minimal overhead.
While **channel-wise pruning** does not necessarily guarantee a corresponding reduction in inference latency—especially given the block-structured memory access and kernel fusion behavior of modern accelerators—the **learned sparsity pattern provides valuable insight** into the model’s internal adaptation behavior. 

Pruning a specific channel in the query projection removes the need to consider the corresponding channel in the key projection. Therefore, the successful **pruning in the query projection** may imply that **computing attention weights does not necessarily require contributions** from all channels. On the other hand, **pruning a value projection** channel could imply that the variation introduced by the attention mechanism along that **output direction is unnecessary for the subsequent MLP processing**.  In such cases, the model **instead relies on the residual connection** to propagate information, implying that the unmodified input is already sufficient to represent that semantic subspace.

In our implementation, masked channels are simulated by explicitly zeroing out their corresponding dimensions during inference, thereby preserving functional equivalence while avoiding custom low-level kernels. Even though this does not reduce actual computation, **it exposes a meaningful sparsity pattern** that can be interpreted and potentially exploited by downstream compression techniques.

Also, to convert the real-valued outer product $$A_{\text{mask}} B_{\text{mask}}$$ into a binary mask, we apply a **sigmoid activation** followed by hard thresholding at a fixed value (e.g., 0.5). Because this thresholding operation is non-differentiable, we adopt the **Straight-Through Estimator (STE)** during training. STE uses the hard-threshold during the forward pass but approximates the gradient as if the sigmoid function had been applied directly, allowing smooth end-to-end optimization of the mask parameters despite their discrete behavior at inference.

Therefore, the final binary mask is constructed as follows:

$$
M= \text{STE}(\sigma((A_{\text{mask}} B_{\text{mask}})^\top))
$$

Through this formulation, **MaskLoRA** unifies two essential objectives in parameter-efficient fine-tuning: **performance** and **sparsity**. It enables the model to adapt effectively while selectively suppressing redundant updates in a structured and interpretable manner.

### 2. Loss formulation

To guide the masked adaptation process, we design a composite **loss function** composed of three components: a **task-specific classification loss**, a **distillation loss** from the teacher model, and a **sparsity-inducing regularization term** on the learned mask.

Let $$f_{\text{student}}(x) \in \mathbb{R}^C$$ and $$f_{\text{teacher}}(x) \in \mathbb{R}^C$$ denote the logits produced by the student and teacher models respectively for an input image $$x$$, where $$C$$ is the number of classes. Let $$\mathbf{y} \in \{0,1\}^C$$ be the one-hot encoded ground-truth label vector, and $$T > 0$$ be a temperature parameter. The **total loss** is defined as:

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{CE}} + \alpha \cdot \mathcal{L}_{\text{distill}} + \beta \cdot \mathcal{L}_{\text{sparsity}}
$$

The **classification loss** $$\mathcal{L}_{\text{CE}}$$ is the standard **cross-entropy loss** computed between the student’s predicted distribution and the one-hot label:

$$
\mathcal{L}_{\text{CE}} = -\sum_{c=1}^Cy_c\log \left(\text{softmax}(f_\text{student})_c \right)
$$

The **distillation loss** $$\mathcal{L}_{\text{distill}}$$ encourages the student model to **preserve the soft output behavior of the original model** before masking was applied. It is computed via the **Kullback–Leibler divergence** between the temperature-scaled output distributions:

$$
\mathcal{L}_{\text{distill}} = T^2 \cdot \text{KL} \left( \text{softmax} \left( \frac{f_{\text{teacher}}}{T} \right) \Big\| \text{softmax} \left( \frac{f_{\text{student}}}{T} \right) \right)
$$

In our setting, the **teacher** is not a separate model but the **unmasked version of the student itself**—that is, the same backbone before any masking or pruning is applied. This constitutes a form of **self-distillation**, where the model is trained to **retain its original predictive behavior** even as adaptation and sparsification are introduced. Such regularization helps mitigate the risk of overfitting to the task-specific data and encourages the adapted model to remain close to the original model’s semantic manifold.

The final term, $$\mathcal{L}_{\text{sparsity}}$$ promotes **mask sparsity**. Specifically, for each transformer block $$i$$, let $$M_{i,q} \in [0, 1]^{ d_h \times d_\text{model}}$$ and $$M_{i,v} \in [0, 1]^{d_h \times d_\text{model}}$$ denote the **soft masks**—i.e., the **output of the sigmoid activations**—applied to the query and value projections, respectively. The sparsity loss is computed as the sum of normalized **$$L_{1,1}$$ norm** of each mask:

$$
\mathcal{L}_{\text{sparsity}} = \sum_{i=1}^L \frac{1}{2}\cdot\left( \frac{1}{d_\text{model}d_h} \sum_{j,k} |M_{i,q}^{(j,k)}| + \frac{1}{d_\text{model}d_h} \sum_{j,k} |M_{i,v}^{(j,k)}| \right)
$$

where $L$ is the number of transformer blocks. This term encourages most mask values to approach zero, thereby pruning redundant adaptation directions, while leaving only a small subset of critical subspaces for task-specific fine-tuning.

Together, the **three terms** define an objective that balances **task performance**, **fidelity to the original model behavior**, and **structural efficiency** in adaptation. By jointly optimizing this loss, the model is encouraged to focus its limited adaptation capacity on dimensions that truly matter, while avoiding unnecessary updates and preserving generalization.

### 3. Training strategy

All experiments were conducted using a **DeiT-Base model** as the backbone. Before conducting full training, we performed **hyperparameter tuning using Optuna**, a widely adopted hyperparameter optimization library based on Bayesian optimization with a Tree-structured Parzen estimator (TPE). Internally, Optuna approximates the posterior over promising regions using a Gaussian mixture model (GMM), and selects sampling candidates by maximizing the expected improvement over observed trials. This approach enables efficient exploration of high-dimensional, continuous search spaces even with limited trial budgets.

We used Optuna to **tune four key hyperparameters**: the **LoRA rank** $$r$$, the **distillation loss coefficient** $$\alpha$$, the **distillation temperature** $$T$$, and the **LoRA scaling factor** $$\gamma$$. The corresponding search spaces were defined as $$r \in \{2, \dots, 8\}$$, $$\alpha \in [0.1, 1.0]$$, $$T \in [2.0, 5.0]$$, $$\gamma \in [0.1, 2.0]$$ with all continuous variables explored using a search step size of 0.0001. The **sparsity regularization weight** $$\beta$$, on the other hand, was **fixed to one of four pre-selected values** $$\{0.01, 0.09, 0.21, 1.00\}$$, and hyperparameter optimization was performed independently for each setting of $$\beta$$. The objective of each Optuna trial was to **maximize validation accuracy**, measured not on the ImageNet validation set, but on a **subset of the ImageNet training set constructed by randomly sampling 50 images** per class from the 1000-class ImageNet-1k training split. To avoid confusion, we emphasize that **no part of the official ImageNet validation set was used in any stage of Optuna tuning**.

**Training** for each trial was conducted on a similarly **constructed 50-shot subset** of the ImageNet training set (i.e., **50 samples per class** drawn at random with fixed seed), using a **maximum of 10 training epochs**. Although both the number of epochs and total trials were relatively small, this was a practical choice to manage computational cost. We did not attempt to exhaustively search for the best setting, but **aimed to identify a set of reasonably effective hyperparameters** for use in subsequent fine-tuning.

Among the trials on each fixed-$$\beta$$ setting, we identified **Pareto-optimal candidates balancing validation accuracy and mask sparsity**—measured by the proportion of entries in the learned binary mask that fell below the hard threshold of 0.5. From the resulting Pareto fronts, we selected **four configurations** that reflected different trade-offs between accuracy and sparsity, with the chosen hyperparameters as follows:

|![hyperparam]({{site.url}}/images/loraskip/hyperparam.png)|
|:--:|
|Table 1. Hyperparameters|

**Using these hyperparameters, we conducted full training** on the entire ImageNet-1k training set. The optimization proceeded in **two phases**. In the **first phase**, both the **LoRA weights and the mask parameters were jointly optimized for 10 epochs**. This joint training phase allowed the model to discover which channels were most relevant for adaptation, while learning a soft mask that reflects the structure of redundant or non-essential adaptation dimensions. In the **second phase**, the **learned mask was frozen for all mask parameters**, and training continued for an **additional 10 epochs** with updates restricted to the **LoRA weights only**. This decoupled training strategy encourages **stability**, and allows the model to **refine the adaptation** within the support already selected by the mask. Furthermore, this process can be considered as **“pretrained model: study all concepts”**, **“self-distilled pruning & fine-tuning: highlighting important concepts”**, **“freeze mask & further tuning: study with only important concepts”** which humans do.

We used the Adam optimizer with a learning rate of $$10^{-4}$$ and a batch size of 192 throughout the whole phase. However, for the configurations with $$\beta = 0.21$$ and $$\beta = 1.00$$, we observed instability in the loss during the second phase. To mitigate this, we reduced the learning rate to $$10^{-5}$$ in these cases, which led to more stable convergence.

# 3. Results

---

Using our proposed method, we apply **structured pruning** to the **DeiT-Base model**. Evaluation is conducted on the **ImageNet-1k validation set** using top-1 accuracy. The original, unpruned DeiT-Base achieves **81.8% accuracy**, which we use as a reference for comparison. Our experiments are designed to understand **how accuracy changes with varying sparsity levels in the query and value projections, how the learned masks are distributed across layers, and how different training strategies affect final performance**. In addition, we report strange phenomenon that emerged during experimentation.

### **1. Accuracy vs. Total Masking Ratio**

|![masking vs acc]({{site.url}}/images/loraskip/masking_vs_acc.jpg)|
|:--:|
|Fig 5. Total masking ratio of query and value projection vs accuracy|

Our first analysis investigates the **relationship between total masking ratio and classification accuracy**. Here, the **“*total masking ratio”***  is defined as the fraction of elements in the **query and value projection matrices that are masked out** (i.e., whose corresponding soft mask value fell below the hard threshold of 0.5) relative to the total number of elements in those matrices.

When the total masking ratio is relatively moderate—specifically 12.93%, 29.88%, and 42.21%—the observed accuracy drops are all below 1.5 percentage points, suggesting that a **some degree of sparsity can be tolerated without significantly degrading performance**. However, at a more aggressive masking ratio of 64.68%, accuracy drops more sharply to 78.04%, indicating that **excessive pruning in these projections impairs model capacity**.

### **2. Layer-Wise Masking Patterns**

|![layerwise_masking_analysis_beta0.01]({{site.url}}/images/loraskip/layerwise_beta0.01.png)|
|:--:|
|Fig 6. layer-wise masking analysis $$\beta$$ = 0.01|

|![layerwise_masking_analysis_beta0.09]({{site.url}}/images/loraskip/layerwise_beta0.09.png)|
|:--:|
|Fig 7. layer-wise masking analysis $$\beta$$ = 0.09|

|![layerwise_masking_analysis_beta0.21]({{site.url}}/images/loraskip/layerwise_beta0.21.png)|
|:--:|
|Fig 8. layer-wise masking analysis $$\beta$$ = 0.21|

|![layerwise_masking_analysis_beta1.00]({{site.url}}/images/loraskip/layerwise_beta1.00.png)|
|:--:|
|Fig 9. layer-wise masking analysis $$\beta$$ = 1.00|

To gain deeper insight into this accuracy degradation, we examined the **distribution of mask values** across transformer layers for each configuration. Each layer in DeiT-Base is indexed from 0 (closest to the input) to 11 (closest to the classification head). **Three key trends** emerge from this analysis.

**First, the model tends to prune the query projections more aggressively than the value projections**. This is consistent with our earlier discussion: **pruning a value projection channel** suggests that the variation introduced by the attention mechanism along that **output direction is deemed unnecessary** for subsequent processing. In such cases, the **residual connection becomes the primary carrier of information**, implying that the original input vector alone suffices to represent the associated semantic feature. Intuitively, this situation is rare—most output directions derived from attention likely carry useful information—so pruning value projections is riskier and thus less frequent.

**Second, the learned masks exhibit a consistent pattern: early layers are pruned more heavily than later layers, with a slight increase in masking observed at the final layer**. This trend aligns with common intuitions in Vision Transformer (ViT) research. **Early layers** are typically responsible for **encoding coarse spatial features that are often redundant or compressible**, while **deeper layers** **capture more abstract, task-specific representations** that are **harder to prune**. The **slight increase in masking at the final layer** may be due to its **direct connection to the classification head**. Unlike generative tasks such as image reconstruction, classification does not require the preservation of all input details. Instead, it relies on a compact set of high-level, discriminative features. at that stage, the network may only require a reduced set of highly informative features, making some pruning acceptable.

**Third, and most critically, we observe that at the highest masking level (64.68%), this structured pruning pattern in the query projections collapses**. Rather than concentrating pruning in early layers, the model exhibits widespread pruning across all layers, including the deeper ones. This breakdown likely results in excessive information loss and explains the sharp drop in accuracy at this setting.

### **3. Effectiveness of Phase 2 Fine-Tuning**

|![two phase training.jpg]({{site.url}}/images/loraskip/two_phase_training.jpg)|
|:--:|
|Fig 10. Impact of Two-Phase Traning|

The **second experiment** evaluates the **utility of Phase 2 fine-tuning**, where the learned masks are frozen and only the LoRA weights are further updated. We find that when the **masking ratio is low**, Phase 2 training **contributes little additional benefit**, likely because the accuracy is already close to the upper bound set by the unpruned DeiT-Base model. In such cases, there is limited room for further improvement through additional fine-tuning. However, at **higher masking levels**, freezing the mask and fine-tuning the adaptation weights yields **nontrivial improvements in accuracy**. This suggests that even after the adaptive subspace has been selected, further refinement within that space can partially recover lost performance. 

### **4. Model Size After Pruning**

|![Accuracy vs Number of Parameters]({{site.url}}/images/loraskip/acc_num_parameter.png)|
|:--:|
|Fig 11. Accuracy vs Number of Parameters|

While query and value projections are successfully pruned, the overall reduction in model parameters is relatively modest. Across the evaluated configurations, **total parameter counts decreased by only 2.11%, 4.89%, 6.90%, and 10.58%**, respectively. This is largely due to the fact that **we did not prune the feed-forward network (FFN) modules,** which dominate the parameter budget in transformer models. Our results imply that although masking attention projections can offer meaningful sparsity, additional gains—especially in model size reduction—will likely require extending the pruning mechanism to FFN layers as well.

In the figure above, we present a comparison between our method and several sparsity-aware baseline approaches. This is consistent with earlier observations: **without pruning the FFN blocks**, even aggressive pruning of attention projections yields **limited parameter savings**. The results suggest that combining **lightweight LoRA-based adaptation with structured FFN pruning** may offer a more promising path for achieving high sparsity without large performance penalties.

### **5. Unexpected Behavior During Hyperparameter Search**

During our experiments, we observed a **counterintuitive phenomenon** when training was restricted to **Phase 1** only (i.e., joint training of weights and masks without subsequent fine-tuning) under aggressive pruning constraints. Specifically, **models trained on a reduced subset of the ImageNet training set**—consisting of 50 randomly sampled images per class—**sometimes outperformed those trained on the full training set in terms of validation accuracy**.

For instance, under the configuration with $$\beta$$ = 0.21, the model trained on the subset achieved 80.06% accuracy, while the same configuration trained on the full dataset achieved 78.78%. A similar pattern emerged at $$\beta$$ = 1.00, where the subset-trained model reached 77.91% accuracy, compared to 74.69% when trained on the full data.

This discrepancy suggests that **limiting the training set size may, paradoxically, lead to better outcomes under certain conditions**. **One hypothesis** is that a smaller dataset results in **fewer gradient updates** for a fixed number of epochs and batch size, **thereby reducing the risk of overfitting the mask parameters**. From this perspective, **mask learning may not require prolonged or large-scale training**. This may also help explain why prior methods that rely on non-learnable pruning strategies often perform surprisingly well in practice.

**Another possibility** is that, although the distillation loss encourages the student to align its predictions with a pre-trained teacher, its **fixed weighting** in the composite objective **may lead to overfitting** when the model is exposed **to a larger and more diverse training set**. In such cases, the student may begin to memorize individual training examples, which can hinder its ability to generalize to unseen data.

# 4. Limitation

---

There are some **limitations** on our method and claim. But we are happy that this means our method has more potential to further improve. Maybe you guys can find the next research idea from here!

- **Increasing parameter**
    - Although we are using the LoRA method for fine-tuning and pruning, two LoRA layers are needed, which makes more parameters.

- **Zero parameters are still there**
    - We are not removing parameters but setting zero using MaskLoRA, so the parameters are still there.

- **Hyperparameter searching**
    - We need to find optimal hyperparameters using Optuna.
    - We are using a subset to find hyperparameters but still need a bunch of time.
    - And saying using a subset, found hyperparameters have the possibility of not being optimal for the model.

- **Not enough experiment (Due to Not enough resources HaHa…)**
    - We only experienced on DeiT-Base model.
    - For tasks, we only tested on ImageNet classification task.

- **Relatively Bad Performance**
    - Compared with other methods, it has relatively bad performance.
    - We think this occurred since the we do not prune & fine-tune FFN and lack of training epoch.

# 5. Conclusion

---

In this work, we explored LoRASKIP which is a **two-phase training strategy that jointly learns both LoRA weights and channel-wise binary masks**, with the goal of identifying task-relevant adaptation subspaces in vision transformers. To the best of our knowledge, this is the **first study** that reports empirical results on **simultaneously training weight updates and pruning masks** within a **Vision Transformer (ViT) backbone**. While prior works have considered LoRA-based adaptation and pruning-based compression separately, we investigated their joint optimization in a unified framework.

Our central **motivation** stemmed from a simple yet underexplored question: **imitating human behavior**, if structured pruning can be seen as imposing a learnable mask, **is it possible**—and perhaps even beneficial—**to optimize such a mask alongside weight adaptation?** In this view, **pruning** is not merely a conventional model compression method applied after pretraining, but **fundamentally the same optimization process as weight adaptation**, just operating under an explicit sparsity constraint.

Admittedly, the scope of **our investigation was limited by practical constraints**: we did not perform exhaustive comparisons across a wide range of baselines, model architectures, or datasets. As such, our findings cannot offer a definitive answer to the general utility of the approach. 

Nevertheless, we believe **this work remains meaningful**. Through a series of controlled experiments on ImageNet-1k, we observed that jointly learning masks and LoRA weights not only yields stable training behavior but also does not inherently degrade performance. We were initially concerned that this joint optimization might lead to unstable training or divergence, especially in early stages. However, the **training process remained stable**, and in several configurations, the joint approach proved surprisingly **robust**.

Moreover, our findings suggest a subtle yet intriguing phenomenon: in highly constrained training setups where aggressive pruning is applied but **further fine-tuning is impractical**, **using a reduced training set may paradoxically improve final performance**. This may be implying that **mask optimization may not always require prolonged training** to be effective.

The practical implications of our method—for instance, its potential to reduce inference latency or compress model size to an industrially meaningful degree—may remain **limited by current hardware constraints**. Nonetheless, this study highlights a direction worth exploring, not because it promises **immediate deployment** gains, but because **it contributes to our conceptual understanding of sparsity, adaptation, and model efficiency**. Even in the absence of overwhelming success, **negative results or inconclusive findings can guide future research** by clarifying which paths may be less fruitful and which hypotheses merit deeper investigation. We hope that the ideas explored here may inform future studies on structured pruning and adaptation in large-scale models such as LLMs, where balancing sparsity and task performance is of growing practical relevance.


---
\[Author\]  
Hyunwoo Jung >> hyunwoojung@postech.ac.kr  
Seobin Lee >> lsb0918@postech.ac.kr  

---